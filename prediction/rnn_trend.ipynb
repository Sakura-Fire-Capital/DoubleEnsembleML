{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import talib\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume (BTC)',\n       'Volume (Currency)', 'Weighted Price'],\n      dtype='object')"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/zed/AI_Lab/DoubleEnsembleML/Data/\"\n",
    "list = [\"BTC\",'DOGE',\"ETC\",'ETH','FIL','LTC','XRP']\n",
    "data = pd.read_csv(path +list[0]+\".csv\")\n",
    "data.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume (BTC)</th>\n      <th>Volume (Currency)</th>\n      <th>Weighted Price</th>\n      <th>H-L</th>\n      <th>O-C</th>\n      <th>3day MA</th>\n      <th>10day MA</th>\n      <th>30day MA</th>\n      <th>Std_dev</th>\n      <th>RSI</th>\n      <th>ATR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30</th>\n      <td>2014-02-06</td>\n      <td>802.50000</td>\n      <td>803.72713</td>\n      <td>767.60884</td>\n      <td>767.60884</td>\n      <td>33.592125</td>\n      <td>2.612999e+04</td>\n      <td>777.860501</td>\n      <td>36.11829</td>\n      <td>-34.89116</td>\n      <td>808.226667</td>\n      <td>807.572468</td>\n      <td>824.918283</td>\n      <td>20.390928</td>\n      <td>34.892078</td>\n      <td>27.788285</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>2014-02-07</td>\n      <td>775.20510</td>\n      <td>775.20510</td>\n      <td>666.35039</td>\n      <td>714.12206</td>\n      <td>64.823212</td>\n      <td>4.640043e+04</td>\n      <td>715.799678</td>\n      <td>108.85471</td>\n      <td>-61.08304</td>\n      <td>790.836280</td>\n      <td>806.333352</td>\n      <td>823.505245</td>\n      <td>42.058537</td>\n      <td>24.758009</td>\n      <td>39.369203</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>2014-02-08</td>\n      <td>711.41118</td>\n      <td>730.00000</td>\n      <td>680.01000</td>\n      <td>699.30255</td>\n      <td>31.042254</td>\n      <td>2.196645e+04</td>\n      <td>707.630659</td>\n      <td>49.99000</td>\n      <td>-12.10863</td>\n      <td>761.410300</td>\n      <td>796.146558</td>\n      <td>819.809884</td>\n      <td>48.505919</td>\n      <td>22.702711</td>\n      <td>40.886460</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2014-02-09</td>\n      <td>692.37535</td>\n      <td>756.09980</td>\n      <td>684.85098</td>\n      <td>689.00000</td>\n      <td>38.291274</td>\n      <td>2.758184e+04</td>\n      <td>720.316621</td>\n      <td>71.24882</td>\n      <td>-3.37535</td>\n      <td>727.011150</td>\n      <td>786.576813</td>\n      <td>815.057658</td>\n      <td>48.591578</td>\n      <td>21.318570</td>\n      <td>45.223940</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>2014-02-10</td>\n      <td>686.83613</td>\n      <td>748.00000</td>\n      <td>550.00000</td>\n      <td>690.20000</td>\n      <td>29.856668</td>\n      <td>1.959739e+04</td>\n      <td>656.382333</td>\n      <td>198.00000</td>\n      <td>3.36387</td>\n      <td>700.808203</td>\n      <td>774.676813</td>\n      <td>809.446639</td>\n      <td>32.643148</td>\n      <td>21.942173</td>\n      <td>67.049091</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2652</th>\n      <td>2021-04-21</td>\n      <td>56500.00000</td>\n      <td>56805.50000</td>\n      <td>53600.00000</td>\n      <td>53815.20000</td>\n      <td>4788.013532</td>\n      <td>2.645352e+08</td>\n      <td>55249.477244</td>\n      <td>3205.50000</td>\n      <td>-2684.80000</td>\n      <td>56153.266667</td>\n      <td>59957.890000</td>\n      <td>57851.923333</td>\n      <td>2276.730403</td>\n      <td>31.587568</td>\n      <td>3594.414995</td>\n    </tr>\n    <tr>\n      <th>2653</th>\n      <td>2021-04-22</td>\n      <td>53815.30000</td>\n      <td>55459.40000</td>\n      <td>50500.00000</td>\n      <td>51730.00000</td>\n      <td>10144.204074</td>\n      <td>5.396125e+08</td>\n      <td>53194.167081</td>\n      <td>4959.40000</td>\n      <td>-2085.30000</td>\n      <td>55345.066667</td>\n      <td>59343.810000</td>\n      <td>57842.593333</td>\n      <td>2012.718937</td>\n      <td>26.873534</td>\n      <td>3789.412853</td>\n    </tr>\n    <tr>\n      <th>2654</th>\n      <td>2021-04-23</td>\n      <td>51709.30000</td>\n      <td>52124.40000</td>\n      <td>47549.30000</td>\n      <td>51178.70000</td>\n      <td>11159.243380</td>\n      <td>5.545896e+08</td>\n      <td>49697.773594</td>\n      <td>4575.10000</td>\n      <td>-530.60000</td>\n      <td>54015.066667</td>\n      <td>58534.440000</td>\n      <td>57755.216667</td>\n      <td>2352.239108</td>\n      <td>25.731360</td>\n      <td>3901.653874</td>\n    </tr>\n    <tr>\n      <th>2655</th>\n      <td>2021-04-24</td>\n      <td>51178.60000</td>\n      <td>51200.00000</td>\n      <td>48746.40000</td>\n      <td>50093.40000</td>\n      <td>4039.459093</td>\n      <td>2.021033e+08</td>\n      <td>50032.268888</td>\n      <td>2453.60000</td>\n      <td>-1085.20000</td>\n      <td>52241.300000</td>\n      <td>57293.590000</td>\n      <td>57718.046667</td>\n      <td>2536.070407</td>\n      <td>23.517675</td>\n      <td>3694.789035</td>\n    </tr>\n    <tr>\n      <th>2656</th>\n      <td>2021-04-25</td>\n      <td>50085.70000</td>\n      <td>50580.40000</td>\n      <td>47000.00000</td>\n      <td>48677.40000</td>\n      <td>4963.986222</td>\n      <td>2.434547e+08</td>\n      <td>49044.194158</td>\n      <td>3580.40000</td>\n      <td>-1408.30000</td>\n      <td>51000.700000</td>\n      <td>56006.030000</td>\n      <td>57677.330000</td>\n      <td>1914.264443</td>\n      <td>20.880925</td>\n      <td>3678.447744</td>\n    </tr>\n  </tbody>\n</table>\n<p>2627 rows × 16 columns</p>\n</div>",
      "text/plain": "            Date         Open         High          Low        Close  \\\n30    2014-02-06    802.50000    803.72713    767.60884    767.60884   \n31    2014-02-07    775.20510    775.20510    666.35039    714.12206   \n32    2014-02-08    711.41118    730.00000    680.01000    699.30255   \n33    2014-02-09    692.37535    756.09980    684.85098    689.00000   \n34    2014-02-10    686.83613    748.00000    550.00000    690.20000   \n...          ...          ...          ...          ...          ...   \n2652  2021-04-21  56500.00000  56805.50000  53600.00000  53815.20000   \n2653  2021-04-22  53815.30000  55459.40000  50500.00000  51730.00000   \n2654  2021-04-23  51709.30000  52124.40000  47549.30000  51178.70000   \n2655  2021-04-24  51178.60000  51200.00000  48746.40000  50093.40000   \n2656  2021-04-25  50085.70000  50580.40000  47000.00000  48677.40000   \n\n      Volume (BTC)  Volume (Currency)  Weighted Price         H-L         O-C  \\\n30       33.592125       2.612999e+04      777.860501    36.11829   -34.89116   \n31       64.823212       4.640043e+04      715.799678   108.85471   -61.08304   \n32       31.042254       2.196645e+04      707.630659    49.99000   -12.10863   \n33       38.291274       2.758184e+04      720.316621    71.24882    -3.37535   \n34       29.856668       1.959739e+04      656.382333   198.00000     3.36387   \n...            ...                ...             ...         ...         ...   \n2652   4788.013532       2.645352e+08    55249.477244  3205.50000 -2684.80000   \n2653  10144.204074       5.396125e+08    53194.167081  4959.40000 -2085.30000   \n2654  11159.243380       5.545896e+08    49697.773594  4575.10000  -530.60000   \n2655   4039.459093       2.021033e+08    50032.268888  2453.60000 -1085.20000   \n2656   4963.986222       2.434547e+08    49044.194158  3580.40000 -1408.30000   \n\n           3day MA      10day MA      30day MA      Std_dev        RSI  \\\n30      808.226667    807.572468    824.918283    20.390928  34.892078   \n31      790.836280    806.333352    823.505245    42.058537  24.758009   \n32      761.410300    796.146558    819.809884    48.505919  22.702711   \n33      727.011150    786.576813    815.057658    48.591578  21.318570   \n34      700.808203    774.676813    809.446639    32.643148  21.942173   \n...            ...           ...           ...          ...        ...   \n2652  56153.266667  59957.890000  57851.923333  2276.730403  31.587568   \n2653  55345.066667  59343.810000  57842.593333  2012.718937  26.873534   \n2654  54015.066667  58534.440000  57755.216667  2352.239108  25.731360   \n2655  52241.300000  57293.590000  57718.046667  2536.070407  23.517675   \n2656  51000.700000  56006.030000  57677.330000  1914.264443  20.880925   \n\n              ATR  \n30      27.788285  \n31      39.369203  \n32      40.886460  \n33      45.223940  \n34      67.049091  \n...           ...  \n2652  3594.414995  \n2653  3789.412853  \n2654  3901.653874  \n2655  3694.789035  \n2656  3678.447744  \n\n[2627 rows x 16 columns]"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data\n",
    "dataset['H-L'] = dataset['High'] - dataset['Low']\n",
    "dataset['O-C'] = dataset['Close'] - dataset['Open']\n",
    "dataset['3day MA'] = dataset['Close'].shift(1).rolling(window = 3).mean()\n",
    "dataset['10day MA'] = dataset['Close'].shift(1).rolling(window = 10).mean()\n",
    "dataset['30day MA'] = dataset['Close'].shift(1).rolling(window = 30).mean()\n",
    "dataset['Std_dev']= dataset['Close'].rolling(5).std()\n",
    "dataset['RSI'] = talib.RSI(dataset['Close'].values, timeperiod = 9)\n",
    "dataset['ATR'] = talib.ATR(dataset['High'].values, dataset['Low'].values, dataset['Close'].values, 7)\n",
    "data  = dataset.dropna(how = \"any\")\n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[\"Weighted Price\"]\n",
    "X = data.drop([\"Date\",\"Weighted Price\"],axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, label, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = label[i+seq_length,:]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Y = Y.values.reshape(-1,1)\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "X = x_scaler.fit_transform(X)\n",
    "Y = y_scaler.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sliding_windows(X, Y, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_first = False\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers)\n",
    "        self.th = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #不手动指定 h 和 c 的话，默认就是 0\n",
    "        # h_0 = torch.zeros(\n",
    "        #     self.num_layers, x.size(1), self.hidden_size)\n",
    "        \n",
    "        # c_0 = torch.zeros(\n",
    "        #     self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # #Propagate input through LSTM\n",
    "        # ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        # ula, (h_out, _) = self.lstm(x)\n",
    "        r_out, (h_n, h_c) = self.lstm(x)  # None 表示 hidden state 会用全0的 state\n",
    "        out_0 = self.th(r_out)\n",
    "        out = self.fc(r_out)\n",
    "        return out\n",
    "        \n",
    "        # h_out = h_out.view(-1, self.hidden_size)\n",
    "        \n",
    "        # out = self.fc(h_out)\n",
    "        # output = self.softmax(out)\n",
    "        # return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([15, 2088, 14]), torch.Size([2088, 1]))"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0.8\n",
    "y_train,y_test = y[:int(y.shape[0]*i)],y[int(y.shape[0]*i):]\n",
    "x_train,x_test = x[:int(x.shape[0]*i)],x[int(x.shape[0]*i):]\n",
    "\n",
    "# lstm: seq, batch, feature\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataX = torch.Tensor(x.transpose(1,0,2))\n",
    "dataY = torch.Tensor(y)\n",
    "trainX = torch.Tensor(x_train.transpose(1,0,2))\n",
    "trainY = torch.Tensor(y_train)\n",
    "testX = torch.Tensor(x_test.transpose(1,0,2))\n",
    "testY = torch.Tensor(y_test)\n",
    "trainX.shape, trainY.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/zed/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([2088, 1])) that is different to the input size (torch.Size([15, 2088, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 1/1000 [00:00<05:48,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.00409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [00:45<02:59,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, loss: 0.00013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [01:30<02:18,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400, loss: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [02:16<01:30,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, loss: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [03:02<00:45,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 800, loss: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:48<00:00,  4.38it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.002\n",
    "device = \"cpu\"\n",
    "input_size = X.shape[1] # The number of expected features in the input x\n",
    "hidden_size = 100        # The number of features in the hidden state h\n",
    "num_layers = 1 # Number of recurrent layers.\n",
    "seq_length = 15\n",
    "num_classes = 1 # output\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "#optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "lstm.train()\n",
    "lstm.to(device)\n",
    "trainX = trainX.to(device)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm(trainX)\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch%200 == 0 :\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-fdef9a1f72f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataY_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdataY_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[1;32m    660\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "\n",
    "lstm.eval()\n",
    "lstm.to(torch.device(device))\n",
    "with torch.no_grad():\n",
    "    dataY_pred = lstm(dataX)\n",
    "\n",
    "dataY_pred = dataY_pred.data.numpy()\n",
    "dataY_truth = dataY.data.numpy()\n",
    "\n",
    "dataY_pred = y_scaler.inverse_transform(dataY_pred)\n",
    "dataY_truth = y_scaler.inverse_transform(dataY_truth)\n",
    "\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=dataY_truth.flatten(),name='Ground Truth'))\n",
    "fig.add_trace(go.Scatter(y=dataY_pred.flatten(),name='Predicted'))\n",
    "\n",
    "fig.update_layout(\n",
    "    shapes = [dict(\n",
    "        x0=len(x_train), x1=len(x_train), y0=0, y1=1, xref='x', yref='paper',\n",
    "        line_width=2)], #在图上划分训练集和测试集\n",
    "    xaxis_rangeslider_visible=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "09b947ba581bcc337bc7c28ed026ab68f0805dc98e756a19c45c9bd5e23e2d3d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}